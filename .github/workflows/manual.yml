# This is a basic workflow that is manually triggered

name: Manual workflow

# Controls when the action will run. Workflow runs when manually triggered using the UI
# or API.
on:
  workflow_dispatch:
    # Inputs the workflow accepts.
    inputs:
      scraping_url:
        # Description to be shown in the UI, instead of 'scraping-url'
        description: 'Scraping URL'
        # Default value if no value is explicitly provided
        default: 'gist.github.com/Iron-Wolf'
        # Input has to be provided for the workflow to run
        required: true
        # The data type of the input
        type: string

# doc : https://docs.github.com/en/actions/learn-github-actions/variables
env:
  base_url: ''

jobs:
  scrape:
    runs-on: ubuntu-latest
    env:
      regex_base_url: '.*\.(\w){2,3}(?=/)'
    steps:
      - uses: actions/checkout@v4
      - name: context values
        run: |
          echo "URL : ${{inputs.scraping_url}}"
          # use grep, because bash regex does not support lookahead (?)
          base_url=$(echo ${{inputs.scraping_url}} | grep -oP $regex_base_url)
          echo "base_url=$base_url" >> $GITHUB_ENV
      - name: Download website
        run: |
          wget \
            --recursive \
            --level=2 \
            --wait=1 \
            --no-clobber \
            --page-requisites \
            --html-extension \
            --convert-links \
            --domains quotes.toscrape.com \
            https://${{ inputs.scraping_url }}
      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v5
        with:
          commit-message: Scrapped result
          title: "URL : ${{env.base_url}}"
          # action generate ".html" files
          body: Visualize here > https://raw.githack.com/Iron-Wolf/proxy-pass/scrape/${{inputs.scraping_url}}.html
          branch: scrape
